{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sweetviz as sv\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"https://mlflow-nhsanv7hcq-uc-a.run.app\")\n",
    "\n",
    "# The credentials created by gcloud auth application-default login can be put in a link below\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/.config/application_default_credentials.json'\n",
    "\n",
    "# Set env details\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'username'\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = 'pot-test-environment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print token, copy and paste in the env variable in the next cell\n",
    "!gcloud auth print-identity-token\n",
    "\n",
    "# The output of gcloud auth print-identity-token is the key here\n",
    "os.environ['MLFLOW_TRACKING_TOKEN'] = \"tokennumber\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "# Define paths\n",
    "#input_folder = '/content/drive/My Drive/Software tests/folder'\n",
    "\n",
    "# Load training data # change to your path\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove rows with all missing values from training and testing data, in case any\n",
    "training_data.dropna(axis=0, how='all', inplace=True)\n",
    "testing_data.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "print(\"Training data:\", training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing target values\n",
    "\n",
    "# Check and drop rows with missing target variable in the training set\n",
    "training_data = training_data.dropna(subset=['r1_iram1622'])\n",
    "print(\"New training data:\", training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Features based on completion thresholds\n",
    "\n",
    "# Create feature threshold benchmark\n",
    "feature_missing_threshold = 0.8\n",
    "\n",
    "# Check for missing values in the training dataset\n",
    "missing_values = training_data.isna().sum() / training_data.count()\n",
    "\n",
    "print(\"Features with missing values more than threshold: \", missing_values[missing_values > feature_missing_threshold])\n",
    "\n",
    "# Create a list of features without missing values\n",
    "features_without_missing_values = missing_values[missing_values < feature_missing_threshold].index.tolist()\n",
    "\n",
    "# Print the list of features without missing values\n",
    "print(\"Features without missing values converted to list for INDEX\", features_without_missing_values)\n",
    "\n",
    "# Take subset of data with updated feature list\n",
    "training_data = training_data[features_without_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data for Features\n",
    "\n",
    "# Calculate the median from the training data\n",
    "median_values = training_data.median()\n",
    "\n",
    "# Fill missing values in the training data with the median\n",
    "training_data = training_data.fillna(median_values)\n",
    "print(\"New training data:\", training_data.shape)\n",
    "\n",
    "# Fill missing values in the testing data with the same median values\n",
    "testing_data = testing_data.fillna(median_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Removal\n",
    "\n",
    "# Calculate z-scores for the target column\n",
    "z_scores = stats.zscore(training_data['r1_iram1622'])\n",
    "\n",
    "# Define a threshold for outlier detection (Z-score > 3 or < -3)\n",
    "threshold = 2\n",
    "\n",
    "# Filter out rows where Z-score exceeds the threshold\n",
    "training_data_no_outliers = training_data[(z_scores < threshold) & (z_scores > -threshold)]\n",
    "\n",
    "print(\"Outliers:\\n\\n\", training_data[(z_scores >= threshold) | (z_scores <= -threshold)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Correlated Features\n",
    "\n",
    "# Remove highly correlated features\n",
    "# Set threshold\n",
    "corr_threshold = 0.9\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = training_data_no_outliers.drop(['r1_iram1622'], axis=1).corr().abs()\n",
    "\n",
    "# Create a mask to identify highly correlated features\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "highly_correlated = [column for column in upper_triangle.columns if any(upper_triangle[column] > corr_threshold)]\n",
    "\n",
    "# Drop highly correlated features\n",
    "training_data_no_outliers.drop(highly_correlated, axis=1, inplace=True)\n",
    "features_without_missing_values = [feature for feature in features_without_missing_values if feature not in highly_correlated]\n",
    "\n",
    "print(\"Features Dropped: \", highly_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "\n",
    "# Split training data into train and test\n",
    "\n",
    "# Define the feature columns (excluding 'r1_iram1622' which is the target)\n",
    "feature_columns = features_without_missing_values.copy()\n",
    "\n",
    "# Split the data into features (X) and the target (y)\n",
    "y = training_data_no_outliers['r1_iram1622']\n",
    "\n",
    "# Remove 'r1_iram1622' from the list of feature columns\n",
    "feature_columns.remove('28CSA_MPA_AVG')\n",
    "\n",
    "X = training_data_no_outliers[feature_columns]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=112)\n",
    "\n",
    "# If you want to use a time-based split\n",
    "# Sort the data based on 'time'\n",
    "training_data_sorted = training_data_no_outliers\n",
    "print(training_data_sorted)\n",
    "\n",
    "# Extract the last 20 samples for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "# If you want to use a time-based split\n",
    "training_data_sorted = training_data_no_outliers\n",
    "\n",
    "# Extract the last 20 samples for testing\n",
    "testing_data = training_data_sorted.iloc[-20:]\n",
    "X_test = testing_data.drop(['r1_iram1622'], axis=1)\n",
    "y_test = testing_data['r1_iram1622']\n",
    "\n",
    "# Extract the remaining data for training\n",
    "training_data = training_data_sorted.iloc[:-20]\n",
    "X_train = training_data.drop(['r1_iram1622'], axis=1)\n",
    "y_train = training_data['r1_iram1622']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# Transform the testing data using the parameters learned from the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune Tail of Features for Reducing Overfitting\n",
    "\n",
    "# Remove features of very low importance\n",
    "# Train RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "threshold_feature_imp = 0.0001\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "low_importance_features = feature_importance[feature_importance < threshold_feature_imp].index.tolist()\n",
    "\n",
    "print(\"Total number of features: \", len(feature_importance))\n",
    "print(\"Count of low importance features removed: \", len(low_importance_features))\n",
    "\n",
    "# Remove features with low importance\n",
    "X_train.drop(low_importance_features, axis=1, inplace=True)\n",
    "X_test.drop(low_importance_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow Run\n",
    "EXPERIMENT_NAME = \"CEMQ1_Challenge\"\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "# Create an MLflow experiment, if not already exists\n",
    "experiment_details = mlflow_client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment_details is not None:\n",
    "    print(f\"Experiment: {EXPERIMENT_NAME} - already Exists\")\n",
    "    experiment_id = experiment_details.experiment_id\n",
    "else:\n",
    "    print(f\"Creating New Experiment: {EXPERIMENT_NAME}\")\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Filter Git-related warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to import Git\", category=UserWarning)\n",
    "\n",
    "# Start an MLflow experiment run\n",
    "print(f\"Beginning MLflow Run CemQ Grid Search for Experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"CemQ Grid Search\") as run:\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"RandomForest\": (\n",
    "            RandomForestRegressor(random_state=42),\n",
    "            {\n",
    "                \"n_estimators\": [100, 200, 300, 400],\n",
    "                \"max_depth\": [5, 10, 15],\n",
    "            },\n",
    "        ),\n",
    "        \"GradientBoosting\": (\n",
    "            GradientBoostingRegressor(random_state=42),\n",
    "            {\n",
    "                \"n_estimators\": [50, 100, 150, 200],\n",
    "                \"learning_rate\": [0.1, 0.01],\n",
    "            },\n",
    "        ),\n",
    "        \"LinearRegression\": (\n",
    "            LinearRegression(),\n",
    "            {},\n",
    "        ),\n",
    "        \"SVM\": (\n",
    "            #SVC(),\n",
    "            SVR(),\n",
    "            {\n",
    "                \"C\": [1, 10],\n",
    "                \"kernel\": [\"linear\", \"rbf\"],\n",
    "            },\n",
    "        ),\n",
    "        \"Ridge\" : (Ridge(random_state=42), {\"alpha\": [0.1, 1.0, 10.0]}),\n",
    "        \"Ridge\" : (Lasso(random_state=42), {\"alpha\": [0.1, 1.0, 10.0]})\n",
    "    }\n",
    "    # Train, predict, and calculate MAE for each model\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "print(\"Beginning Grid Search ....\\n\\n\")\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data using the best model\n",
    "    top_model = grid_search.best_estimator_\n",
    "    y_pred = top_model.predict(X_test)\n",
    "    \n",
    "    # Calculate MAE for the model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Log MAE and best hyperparameters for each model in MLflow\n",
    "    mlflow.log_metric(f\"{model_name}_test_MAE\", mae)\n",
    "    mlflow.log_param(f\"{model_name}_best_params\", grid_search.best_params_)\n",
    "    print(\"Model Name:\", model_name, \"; Test MAE:\", mae, \"; Best Params:\", grid_search.best_params_)\n",
    "\n",
    "    # Choose the best model based on the lowest MAE\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_model_name = model_name\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Completion of Grid Search ....\\n\\n\")\n",
    "    print(\"Best Model Name:\", best_model_name)\n",
    "    print(\"Best Model:\", best_model)\n",
    "    print(\"Best Model Test MAE:\", best_mae)\n",
    "\n",
    "    # Log the best model name and its MAE in MLflow\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_metric(\"best_model_test_MAE\", best_mae)\n",
    "\n",
    "    # Generate predictions using the best model\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "\n",
    "    # Evaluate the regression model\n",
    "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "    # r2 = r2_score(y_test, data_predict_y)  # Uncomment if r2_score is needed\n",
    "\n",
    "    # Log metrics in MLflow\n",
    "    mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "    # mlflow.log_metric(\"validation_r2_score\", r2)  # Uncomment if r2_score is logged\n",
    "\n",
    "    # Log the model in MLflow\n",
    "    mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n",
    "\n",
    "# Log run information\n",
    "run_id = run.info.run_id\n",
    "print(\"Run ID: {}\".format(run_id))\n",
    "# Register the model in MLflow\n",
    "model_uri = \"runs:/{}/model\".format(run_id)\n",
    "mv = mlflow.register_model(model_uri, \"CemQ28Challenge_Model\")\n",
    "print(\"Name: {}\".format(mv.name))\n",
    "print(\"Version: {}\".format(mv.version))\n",
    "\n",
    "# Load the registered model as a PyFuncModel\n",
    "logged_model = \"runs:/{}/artifacts/model\".format(run_id)\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Moving Average Method as Base MAE\n",
    "\n",
    "# Start an MLFlow experiment run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"CemQ Moving Average - Base Model\") as run:\n",
    "    # Define the moving average function\n",
    "    def moving_average_forecast(data, window_size):\n",
    "        forecast = []\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            window = data[i:i + window_size]\n",
    "            forecast.append(sum(window) / window_size)\n",
    "        return forecast\n",
    "\n",
    "    # Define the window size for the moving average\n",
    "    window_size = 8  # Adjust and test with multiple values\n",
    "\n",
    "    # Calculate moving average forecast on the training set\n",
    "    train_forecast = moving_average_forecast(y_train, window_size)\n",
    "\n",
    "    # Calculate moving average forecast on the test set\n",
    "    test_forecast = moving_average_forecast(y_test, window_size)\n",
    "\n",
    "    # Compute MAE for the moving average forecast on the training set\n",
    "    train_mae = mean_absolute_error(y_train[window_size - 1:], train_forecast)\n",
    "\n",
    "    # Compute MAE for the moving average forecast on the test set\n",
    "    test_mae = mean_absolute_error(y_test[window_size - 1:], test_forecast)\n",
    "    # Print the Moving Average Train and Test MAE\n",
    "    print(\"Moving Average Train MAE:\", train_mae)\n",
    "    print(\"Moving Average Test MAE:\", test_mae)\n",
    "\n",
    "    # Log the experiment metrics to MLFlow\n",
    "    mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Start an MLFlow experiment run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"CemQ - ARIMA\") as run:\n",
    "    # Create a sample time series DataFrame\n",
    "    dates = pd.date_range(start='2023-01-01', periods=len(y_train), freq='D')\n",
    "    train_data = pd.DataFrame({'Date': dates, 'Value': y_train})\n",
    "    train_data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Define the ARIMA model\n",
    "    p, d, q = 0, 1, 4\n",
    "    arima_model = ARIMA(train_data['Value'], order=(p, d, q))\n",
    "\n",
    "    # Fit the ARIMA model\n",
    "    arima_model_fit = arima_model.fit()\n",
    "\n",
    "    # Forecast using the ARIMA model on the training set\n",
    "    arima_train_forecast = arima_model_fit.predict(start=0, end=len(y_train) - 1)\n",
    "\n",
    "    # Forecast using the ARIMA model on the test set\n",
    "    forecast_steps = len(y_test)\n",
    "    arima_test_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "    # Calculate MAE for the ARIMA forecast on the test and train sets\n",
    "    train_mae = mean_absolute_error(y_train, arima_train_forecast)\n",
    "    test_mae = mean_absolute_error(y_test, arima_test_forecast)\n",
    "\n",
    "    # Print the ARIMA Train and Test MAE\n",
    "    print(\"ARIMA Test MAE:\", test_mae)\n",
    "    print(\"ARIMA Train MAE:\", train_mae)\n",
    "\n",
    "    # Log the experiment results to MLFlow\n",
    "    mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "    mlflow.log_param(\"best_model\", \"ARIMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression with RFE\n",
    "# Start an MLFlow experiment run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"CemQ - RFE\") as run:\n",
    "    # Try linear regression with recursive feature elimination\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Initialize a Linear Regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Initialize RFE with the Linear Regression model\n",
    "    # Set the number of features to select (n_features_to_select) as per your requirement\n",
    "    rfe = RFE(model, n_features_to_select=20)  # Adjust the number of features as needed\n",
    "\n",
    "    # Fit RFE to your training data X_train and target variable y_train\n",
    "    rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Print the selected features\n",
    "    selected_features = X_train.columns[rfe.support_]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    # Train a Linear Regression model using the selected features\n",
    "    model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test[selected_features])\n",
    "    y_train_pred = model.predict(X_train[selected_features])\n",
    "\n",
    "    # Calculate the performance metric (e.g., mean_absolute_error)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    print(\"Training MAE with selected features:\", train_mae)\n",
    "    print(\"Test MAE with selected features:\", test_mae)\n",
    "    # Log Experiment to MLflow\n",
    "    mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "    mlflow.log_param(\"best_model\", \"Recursive Feature Elimination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model with RFE Based Features Selection\n",
    "selected_features = selected_features.copy()\n",
    "\n",
    "# Keep only selected features\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"RFE Feature - Grid Search\") as run:\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"RandomForest\": (\n",
    "            RandomForestRegressor(random_state=42),\n",
    "            {\n",
    "                \"n_estimators\": [100, 200, 300, 400],\n",
    "                \"max_depth\": [5, 10, 15],\n",
    "            },\n",
    "        ),\n",
    "        \"GradientBoosting\": (\n",
    "            GradientBoostingRegressor(random_state=42),\n",
    "            {\n",
    "                \"n_estimators\": [50, 100, 150, 200],\n",
    "                \"learning_rate\": [0.1, 0.01],\n",
    "            },\n",
    "        ),\n",
    "        \"LinearRegression\": (\n",
    "            LinearRegression(),\n",
    "            {},\n",
    "        ),\n",
    "        \"SVM\": (\n",
    "            SVR(),\n",
    "            {\n",
    "                \"C\": [1, 10],\n",
    "                \"kernel\": [\"linear\", \"rbf\"],\n",
    "            },\n",
    "        ),\n",
    "        \"Ridge\" : (Ridge(random_state=42), {\"alpha\": [0.1, 1.0, 10.0]}),\n",
    "        \"Ridge\" : (Lasso(random_state=42), {\"alpha\": [0.1, 1.0, 10.0]})\n",
    "    }\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "print(\"Beginning Grid Search ....\\n\\n\")\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data using the best model\n",
    "    top_model = grid_search.best_estimator_\n",
    "    y_pred = top_model.predict(X_test)\n",
    "    \n",
    "    # Calculate MAE for the model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Log MAE and best hyperparameters for each model in MLflow\n",
    "    mlflow.log_metric(f\"{model_name}_test_MAE\", mae)\n",
    "    mlflow.log_param(f\"{model_name}_best_params\", grid_search.best_params_)\n",
    "    print(\"Model Name:\", model_name, \"; Test MAE:\", mae, \"; Best Params:\", grid_search.best_params_)\n",
    "\n",
    "    # Choose the best model based on the lowest MAE\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_model_name = model_name\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Completion of Grid Search ....\\n\\n\")\n",
    "    print(\"Best Model Name:\", best_model_name)\n",
    "    print(\"Best Model:\", best_model)\n",
    "    print(\"Best Model Test MAE:\", best_mae)\n",
    "\n",
    "    # Log the best model name and its MAE in MLflow\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_metric(\"best_model_test_MAE\", best_mae)\n",
    "\n",
    "    # Generate predictions using the best model\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "    train_predictions = best_model.predict(X_train)\n",
    "\n",
    "    # Evaluate the regression model\n",
    "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "    # r2 = r2_score(y_test, data_predict_y)  # Uncomment if r2_score is needed\n",
    "\n",
    "    # Log metrics in MLflow\n",
    "    mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "    # mlflow.log_metric(\"validation_r2_score\", r2)  # Uncomment if r2_score is logged\n",
    "\n",
    "    # Log the model in MLflow\n",
    "    mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n",
    "\n",
    "# Log run information\n",
    "run_id = run.info.run_id\n",
    "print(\"Run ID: {}\".format(run_id))\n",
    "# Register the model in MLflow\n",
    "model_uri = \"runs:/{}/model\".format(run_id)\n",
    "mv = mlflow.register_model(model_uri, \"CemQ280Challenge_Model\")\n",
    "print(\"Name: {}\".format(mv.name))\n",
    "print(\"Version: {}\".format(mv.version))\n",
    "\n",
    "# Load the registered model as a PyFuncModel\n",
    "#logged_model = \"runs:/{}/artifacts/model\".format(run_id)\n",
    "#loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blend models to reduce overfitting in gridsearch\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"Blended Models + Grid Search\") as run:\n",
    "    # Initialize models with their hyperparameters for Grid Search\n",
    "    models = {\n",
    "        \"RandomForest\": (\n",
    "            RandomForestRegressor(random_state=42),\n",
    "            {\"n_estimators\": [100, 200, 300, 400], \"max_depth\": [5, 10, 15]}\n",
    "        ),\n",
    "        \"GradientBoosting\": (\n",
    "            GradientBoostingRegressor(random_state=42),\n",
    "            {\"n_estimators\": [50, 100, 150, 200], \"learning_rate\": [0.1, 0.01]}\n",
    "        ),\n",
    "        \"LinearRegression\": (LinearRegression(), {}),\n",
    "        \"SVM\": (\n",
    "            SVR(),\n",
    "            {\"C\": [1, 10], \"kernel\": [\"linear\", \"rbf\"]}\n",
    "        ),\n",
    "        \"Ridge\": (\n",
    "            Ridge(random_state=42),\n",
    "            {\"alpha\": [0.1, 1.0, 10.0]}\n",
    "        ),\n",
    "        \"Lasso\": (\n",
    "            Lasso(random_state=42),\n",
    "            {\"alpha\": [0.1, 1.0, 10.0]}\n",
    "        )\n",
    "    }\n",
    "    # Initialize variables to store predictions on train and test sets\n",
    "    train_predictions = np.zeros(X_train.shape[0])\n",
    "    test_predictions = np.zeros(X_test.shape[0])\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    top_model = grid_search.best_estimator_\n",
    "    y_pred_test = top_model.predict(X_test)\n",
    "    y_pred_train = top_model.predict(X_train)\n",
    "\n",
    "    test_predictions += y_pred_test\n",
    "    train_predictions += y_pred_train\n",
    "\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "# Average predictions across all models\n",
    "test_predictions /= len(models)\n",
    "train_predictions /= len(models)\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Training MAE with Blended model:\", train_mae)\n",
    "print(\"Test MAE with Blended model:\", test_mae)\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_metric(\"MAE_test\", test_mae)\n",
    "mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "mlflow.log_param(\"best_model\", \"Blended model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([1, 5, 10], dtype=float)\n",
    "test /= 3\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Base Model on Test Data\n",
    "\n",
    "# Start an MLflow experiment run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"Cem Test Data - Base Model\") as run:\n",
    "    # Define the moving average function\n",
    "    def moving_average_forecast(data, window_size):\n",
    "        forecast = []\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            window = data[i:i + window_size]\n",
    "            forecast.append(sum(window) / window_size)\n",
    "        return forecast\n",
    "    # Drop rows with missing or non-convertible values\n",
    "    training_data = training_data.dropna(subset=[\"r1_iram1622\"])\n",
    "\n",
    "    # Calculate moving average forecast on the cleaned training set\n",
    "    train_forecast = moving_average_forecast(training_data[\"r1_iram1622\"], window_size)\n",
    "\n",
    "    # Apply the same preprocessing steps to the testing data if necessary\n",
    "\n",
    "    # Calculate moving average forecast on the test set using its available features\n",
    "    # Assuming \"28CSA_MPA_AVG\" column is not present in the testing data\n",
    "    test_forecast = moving_average_forecast(train_forecast, window_size=8)\n",
    "    print(test_forecast)\n",
    "\n",
    "    # Compute MAE for the moving average forecast on the training set\n",
    "    train_mae = mean_absolute_error(training_data[\"r1_iram1622\"][window_size - 1:], train_forecast)\n",
    "\n",
    "    # No need to compute MAE for the test set since it's a forecasted column\n",
    "\n",
    "    # Print the Moving Average Train MAE\n",
    "    print(\"Moving Average Train MAE:\", train_mae)\n",
    "\n",
    "    # Log Experiment to MLflow\n",
    "    mlflow.log_metric(\"MAE_train\", train_mae)\n",
    "    mlflow.log_param(\"best_model\", \"Moving Average\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
